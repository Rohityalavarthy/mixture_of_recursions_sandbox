# -*- coding: utf-8 -*-
"""MoR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1n4K752fH4Yr6PmyG1XP6HSgNlKGum4_0
"""

# Mixture-of-Recursions (MoR) Implementation for Google Colab
# Based on "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation" (NeurIPS 2025)

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import math
import time
from typing import Optional, Tuple, Union
from dataclasses import dataclass

#random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

print("PyTorch version:", torch.__version__)
print("Device available:", "CUDA" if torch.cuda.is_available() else "CPU")


class RouterBase(nn.Module):
    """Base router class for MoR"""
    def __init__(self, hidden_size, num_experts=None, initializer_range=0.02):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_experts = num_experts or 1
        self.initializer_range = initializer_range

    def forward(self, x):
        raise NotImplementedError

class LinearRouter(RouterBase):
    """Simple linear router for expert or token choice"""
    def __init__(self, hidden_size, num_experts=1, initializer_range=0.02):
        super().__init__(hidden_size, num_experts, initializer_range)
        self.router = nn.Linear(hidden_size, num_experts, bias=False)
        self.router.weight.data.normal_(mean=0.0, std=initializer_range)

    def forward(self, x):
        return self.router(x)

class MLPRouter(RouterBase):
    """MLP-based router for more sophisticated routing decisions"""
    def __init__(self, hidden_size, num_experts=1, initializer_range=0.02):
        super().__init__(hidden_size, num_experts, initializer_range)
        self.router = nn.Sequential(
            nn.Linear(hidden_size, hidden_size * 2, bias=False),
            nn.GELU(),
            nn.Linear(hidden_size * 2, num_experts, bias=False)
        )
        for layer in self.router:
            if isinstance(layer, nn.Linear):
                layer.weight.data.normal_(mean=0.0, std=initializer_range)

    def forward(self, x):
        return self.router(x)

@dataclass
class MoROutput:
    """Output structure for MoR layers"""
    hidden_states: torch.Tensor
    router_logits: Optional[torch.Tensor] = None
    selected_tokens: Optional[torch.Tensor] = None
    routing_weights: Optional[torch.Tensor] = None
    load_balancing_loss: Optional[torch.Tensor] = None

class SimpleTransformerBlock(nn.Module):
    """Simplified Transformer block for MoR recursion"""
    def __init__(self, hidden_size, num_heads=8, intermediate_size=None, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads
        self.intermediate_size = intermediate_size or hidden_size * 4

        # Multi-head attention
        self.q_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.k_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.v_proj = nn.Linear(hidden_size, hidden_size, bias=False)
        self.o_proj = nn.Linear(hidden_size, hidden_size, bias=False)

        # Feed-forward network
        self.gate_proj = nn.Linear(hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, hidden_size, bias=False)

        # Layer norms
        self.input_layernorm = nn.LayerNorm(hidden_size, eps=1e-6)
        self.post_attention_layernorm = nn.LayerNorm(hidden_size, eps=1e-6)

        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states, attention_mask=None, position_ids=None):
        # Self-attention
        residual = hidden_states
        hidden_states = self.input_layernorm(hidden_states)

        batch_size, seq_len, _ = hidden_states.shape

        # Project to Q, K, V
        query_states = self.q_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = self.k_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        value_states = self.v_proj(hidden_states).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

        # Attention computation
        attention_scores = torch.matmul(query_states, key_states.transpose(-2, -1)) / math.sqrt(self.head_dim)

        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        attention_output = torch.matmul(attention_probs, value_states)
        attention_output = attention_output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)
        attention_output = self.o_proj(attention_output)

        # Add residual
        hidden_states = residual + self.dropout(attention_output)

        # Feed-forward
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)

        gate_output = F.silu(self.gate_proj(hidden_states))
        up_output = self.up_proj(hidden_states)
        intermediate_output = gate_output * up_output
        hidden_states = self.down_proj(intermediate_output)

        # Add residual
        hidden_states = residual + self.dropout(hidden_states)

        return hidden_states

class ExpertChoiceMoRLayer(nn.Module):
    """Expert-choice MoR layer - experts choose which tokens to process"""
    def __init__(self, hidden_size, num_recursions=3, num_heads=8, capacity_factor=1.0,
                 router_type="linear", alpha=0.1, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_recursions = num_recursions
        self.capacity_factor = capacity_factor
        self.alpha = alpha

        # Shared recursion blocks
        self.recursion_blocks = nn.ModuleList([
            SimpleTransformerBlock(hidden_size, num_heads, dropout=dropout)
            for _ in range(num_recursions)
        ])

        # Routers for each recursion step
        if router_type == "linear":
            self.routers = nn.ModuleList([
                LinearRouter(hidden_size, num_experts=1)
                for _ in range(num_recursions)
            ])
        else:
            self.routers = nn.ModuleList([
                MLPRouter(hidden_size, num_experts=1)
                for _ in range(num_recursions)
            ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states, attention_mask=None, position_ids=None):
        batch_size, seq_len, hidden_dim = hidden_states.shape
        total_output = hidden_states.clone()

        all_router_logits = []
        total_load_balancing_loss = 0.0

        # Process through each recursion step
        for step, (block, router) in enumerate(zip(self.recursion_blocks, self.routers)):
            # Compute routing scores
            router_logits = router(hidden_states)  # [batch_size, seq_len, 1]
            router_probs = torch.sigmoid(router_logits) * self.alpha

            all_router_logits.append(router_logits)

            # Top-k selection (capacity-based)
            top_k = max(1, int(self.capacity_factor * seq_len))
            weights, selected_indices = torch.topk(router_probs.squeeze(-1), top_k, dim=-1, sorted=False)
            selected_indices, sort_indices = torch.sort(selected_indices, dim=-1)
            weights = torch.gather(weights, dim=-1, index=sort_indices)

            # Gather selected tokens
            selected_indices_expanded = selected_indices.unsqueeze(-1).expand(-1, -1, hidden_dim)
            selected_tokens = torch.gather(hidden_states, dim=1, index=selected_indices_expanded)

            # Create attention mask for selected tokens
            if attention_mask is not None:
                 # Assuming attention_mask is causal and we only need to slice it
                 # This part assumes a simple causal mask. More complex masks might require different handling.
                 # A simple approach for a causal mask on selected tokens is to apply the original mask
                 # conceptually, but since we only have selected tokens, we need a mask of the size
                 # of the selected tokens. For simplicity with a causal mask assumption, we'll create
                 # a new causal mask for the length of selected tokens.
                 selected_seq_len = selected_tokens.size(1)
                 selected_attention_mask = torch.triu(torch.ones(selected_seq_len, selected_seq_len, device=hidden_states.device), diagonal=1)
                 selected_attention_mask = selected_attention_mask.masked_fill(selected_attention_mask == 1, float('-inf')).unsqueeze(0).unsqueeze(0)
            else:
                 selected_attention_mask = None


            # Process through recursion block
            processed_tokens = block(selected_tokens, attention_mask=selected_attention_mask)

            # Weight the processed tokens
            weighted_tokens = processed_tokens * weights.unsqueeze(-1)

            # Scatter back to original positions
            total_output.scatter_add_(dim=1, index=selected_indices_expanded, src=weighted_tokens)

            # Compute load balancing loss (encourage balanced expert usage)
            if self.training:
                expert_usage = router_probs.mean()  # Mean usage across batch and sequence
                load_balancing_loss = expert_usage * (1 - expert_usage)  # Encourage 0.5 usage
                total_load_balancing_loss += load_balancing_loss

        return MoROutput(
            hidden_states=total_output,
            router_logits=torch.stack(all_router_logits, dim=0),
            load_balancing_loss=total_load_balancing_loss / len(self.recursion_blocks) if self.training else None
        )

class TokenChoiceMoRLayer(nn.Module):
    """Token-choice MoR layer - tokens choose their recursion depth"""
    def __init__(self, hidden_size, num_recursions=3, num_heads=8,
                 router_type="linear", alpha=1.0, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_recursions = num_recursions
        self.alpha = alpha

        # Shared recursion blocks
        self.recursion_blocks = nn.ModuleList([
            SimpleTransformerBlock(hidden_size, num_heads, dropout=dropout)
            for _ in range(num_recursions)
        ])

        # Single router that assigns recursion depth to each token
        if router_type == "linear":
            self.router = LinearRouter(hidden_size, num_experts=num_recursions)
        else:
            self.router = MLPRouter(hidden_size, num_experts=num_recursions)

        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states, attention_mask=None, position_ids=None):
        batch_size, seq_len, hidden_dim = hidden_states.shape

        # Get routing decisions for all tokens
        router_logits = self.router(hidden_states)  # [batch_size, seq_len, num_recursions]
        router_probs = F.softmax(router_logits, dim=-1) * self.alpha

        # Token-choice: each token gets assigned to one recursion depth
        _, token_assignments = torch.topk(router_probs, 1, dim=-1)  # [batch_size, seq_len, 1]
        token_assignments = token_assignments.squeeze(-1)  # [batch_size, seq_len]

        # Get routing weights
        routing_weights = torch.gather(router_probs, dim=-1, index=token_assignments.unsqueeze(-1))

        final_output = hidden_states.clone()
        load_balancing_loss = 0.0

        # Process tokens through their assigned recursion depths
        for depth in range(self.num_recursions):
            # Find tokens assigned to this depth
            depth_mask = (token_assignments == depth)  # [batch_size, seq_len]

            if not depth_mask.any():
                continue

            # Process through all blocks up to assigned depth
            current_hidden = hidden_states.clone()
            for block_idx in range(depth + 1):
                current_hidden = self.recursion_blocks[block_idx](current_hidden, attention_mask=attention_mask)

            # Update only the tokens assigned to this depth
            depth_mask_expanded = depth_mask.unsqueeze(-1).expand(-1, -1, hidden_dim)
            depth_weights = routing_weights.squeeze(-1).unsqueeze(-1).expand(-1, -1, hidden_dim)

            # Weighted update for tokens at this depth
            final_output = torch.where(depth_mask_expanded,
                                     final_output + current_hidden * depth_weights,
                                     final_output)

        # Compute load balancing loss
        if self.training:
            # Encourage balanced distribution across recursion depths
            depth_distribution = router_probs.mean(dim=(0, 1))  # [num_recursions]
            uniform_target = torch.ones_like(depth_distribution) / self.num_recursions
            load_balancing_loss = F.kl_div(depth_distribution.log(), uniform_target, reduction='sum')

        return MoROutput(
            hidden_states=final_output,
            router_logits=router_logits,
            selected_tokens=token_assignments,
            routing_weights=routing_weights,
            load_balancing_loss=load_balancing_loss if self.training else None
        )

class SimpleMoRModel(nn.Module):
    """Complete MoR model for classification tasks"""
    def __init__(self, vocab_size, hidden_size, num_classes, num_layers=6, num_recursions=3,
                 num_heads=8, max_seq_len=512, mor_type="expert_choice",
                 capacity_factor=1.0, alpha=0.1, dropout=0.1):
        super().__init__()
        self.hidden_size = hidden_size
        self.num_layers = num_layers
        self.num_recursions = num_recursions
        self.mor_type = mor_type

        # Embeddings
        self.token_embeddings = nn.Embedding(vocab_size, hidden_size)
        self.position_embeddings = nn.Embedding(max_seq_len, hidden_size)

        # MoR layers
        self.layers = nn.ModuleList()
        for _ in range(num_layers):
            if mor_type == "expert_choice":
                layer = ExpertChoiceMoRLayer(
                    hidden_size, num_recursions, num_heads,
                    capacity_factor, "linear", alpha, dropout
                )
            else:  # token_choice
                layer = TokenChoiceMoRLayer(
                    hidden_size, num_recursions, num_heads,
                    "linear", alpha, dropout
                )
            self.layers.append(layer)

        # Output layers
        self.final_norm = nn.LayerNorm(hidden_size, eps=1e-6)
        self.classifier = nn.Linear(hidden_size, num_classes)
        self.dropout = nn.Dropout(dropout)

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)
            if module.bias is not None:
                torch.nn.init.zeros_(module.bias)
        elif isinstance(module, nn.Embedding):
            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)

    def create_attention_mask(self, input_ids, seq_len):
        """Create causal attention mask"""
        mask = torch.triu(torch.ones(seq_len, seq_len, device=input_ids.device), diagonal=1)
        mask = mask.masked_fill(mask == 1, float('-inf'))
        return mask.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len, seq_len]

    def forward(self, input_ids, labels=None):
        batch_size, seq_len = input_ids.shape
        device = input_ids.device

        # Create embeddings
        position_ids = torch.arange(seq_len, device=device).unsqueeze(0).expand(batch_size, -1)
        token_embeds = self.token_embeddings(input_ids)
        pos_embeds = self.position_embeddings(position_ids)
        hidden_states = token_embeds + pos_embeds
        hidden_states = self.dropout(hidden_states)

        # Create attention mask (full sequence length)
        attention_mask = self.create_attention_mask(input_ids, seq_len)

        # Process through MoR layers
        total_load_balancing_loss = 0.0
        all_router_outputs = []

        for layer in self.layers:
            # Pass the full attention mask to the layer
            output = layer(hidden_states, attention_mask=attention_mask)
            hidden_states = output.hidden_states
            all_router_outputs.append(output)

            if output.load_balancing_loss is not None:
                total_load_balancing_loss += output.load_balancing_loss

        # Final processing
        hidden_states = self.final_norm(hidden_states)

        # Pool for classification (use last token)
        pooled_output = hidden_states[:, -1, :]  # [batch_size, hidden_size]
        logits = self.classifier(pooled_output)

        # Compute loss if labels provided
        loss = None
        if labels is not None:
            loss_fn = nn.CrossEntropyLoss()
            loss = loss_fn(logits, labels)

            # Add load balancing loss
            if total_load_balancing_loss > 0:
                loss += 0.01 * total_load_balancing_loss  # Weight the auxiliary loss

        return {
            'logits': logits,
            'loss': loss,
            'load_balancing_loss': total_load_balancing_loss,
            'router_outputs': all_router_outputs
        }

# ====== DATA AND TRAINING UTILITIES ======

def create_synthetic_sequence_data(num_samples=1000, seq_len=128, vocab_size=1000, num_classes=4):
    """Create synthetic sequence classification data"""
    # Create sequences with different patterns for different classes
    X_list = []
    y_list = []

    for class_idx in range(num_classes):
        samples_per_class = num_samples // num_classes

        # Create class-specific patterns
        for _ in range(samples_per_class):
            # Base sequence
            sequence = torch.randint(1, vocab_size // 2, (seq_len,))

            # Add class-specific patterns at different positions
            if class_idx == 0:  # Pattern at beginning
                sequence[:10] = torch.arange(100, 110)
            elif class_idx == 1:  # Pattern in middle
                mid = seq_len // 2
                sequence[mid:mid+10] = torch.arange(200, 210)
            elif class_idx == 2:  # Pattern at end
                sequence[-10:] = torch.arange(300, 310)
            else:  # Scattered pattern
                positions = torch.randint(0, seq_len, (5,))
                sequence[positions] = torch.arange(400, 405)

            X_list.append(sequence)
            y_list.append(class_idx)

    X = torch.stack(X_list)
    y = torch.tensor(y_list, dtype=torch.long)

    # Shuffle
    perm = torch.randperm(len(X))
    X = X[perm]
    y = y[perm]

    return X, y

def train_mor_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):
    """Training function for MoR model"""
    device = next(model.parameters()).device
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

    train_losses = []
    val_accuracies = []
    load_balancing_losses = []

    for epoch in range(num_epochs):
        # Training
        model.train()
        epoch_loss = 0
        epoch_lb_loss = 0
        num_batches = 0

        for batch_x, batch_y in train_loader:
            batch_x, batch_y = batch_x.to(device), batch_y.to(device)

            optimizer.zero_grad()

            outputs = model(batch_x, labels=batch_y)
            loss = outputs['loss']

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()

            epoch_loss += loss.item()
            epoch_lb_loss += outputs['load_balancing_loss'].item() if outputs['load_balancing_loss'] is not None else 0
            num_batches += 1

        scheduler.step()

        # Validation
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for batch_x, batch_y in val_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                outputs = model(batch_x)
                predictions = torch.argmax(outputs['logits'], dim=-1)
                correct += (predictions == batch_y).sum().item()
                total += batch_y.size(0)

        val_accuracy = 100 * correct / total
        avg_loss = epoch_loss / num_batches
        avg_lb_loss = epoch_lb_loss / num_batches

        train_losses.append(avg_loss)
        val_accuracies.append(val_accuracy)
        load_balancing_losses.append(avg_lb_loss)

        if epoch % 10 == 0:
            print(f'Epoch [{epoch+1}/{num_epochs}]')
            print(f'  Train Loss: {avg_loss:.4f}')
            print(f'  Load Balancing Loss: {avg_lb_loss:.4f}')
            print(f'  Val Accuracy: {val_accuracy:.2f}%')
            print(f'  Learning Rate: {scheduler.get_last_lr()[0]:.6f}')

    return train_losses, val_accuracies, load_balancing_losses

def analyze_routing_patterns(model, data_loader, num_samples=5):
    """Analyze routing patterns in the trained model"""
    model.eval()
    device = next(model.parameters()).device

    routing_stats = {'expert_choice': [], 'token_choice': []}

    with torch.no_grad():
        for i, (batch_x, batch_y) in enumerate(data_loader):
            if i >= num_samples:
                break

            batch_x = batch_x.to(device)
            outputs = model(batch_x)

            for layer_idx, router_output in enumerate(outputs['router_outputs']):
                if hasattr(router_output, 'selected_tokens') and router_output.selected_tokens is not None:
                    # Token choice routing
                    assignments = router_output.selected_tokens.cpu().numpy()
                    routing_stats['token_choice'].append({
                        'layer': layer_idx,
                        'assignments': assignments,
                        'weights': router_output.routing_weights.cpu().numpy() if router_output.routing_weights is not None else None
                    })
                elif hasattr(router_output, 'router_logits'):
                    # Expert choice routing
                    logits = router_output.router_logits.cpu().numpy()
                    routing_stats['expert_choice'].append({
                        'layer': layer_idx,
                        'logits': logits
                    })

    return routing_stats

def main():
    """Main function to run MoR experiment"""
    print("üöÄ Starting Mixture-of-Recursions Experiment")
    print("=" * 60)

    # Configuration
    config = {
        'vocab_size': 1000,
        'hidden_size': 64,
        'num_classes': 4,
        'num_layers': 4,
        'num_recursions': 2,
        'num_heads': 8,
        'seq_len': 64,
        'max_seq_len': 256,
        'num_samples': 2000,
        'batch_size': 32,
        'num_epochs': 4,
        'learning_rate': 0.001,
        'mor_type': 'expert_choice',  # or 'token_choice'
        'capacity_factor': 0.8,
        'alpha': 0.1,
        'dropout': 0.1
    }

    print("Configuration:")
    for key, value in config.items():
        print(f"  {key}: {value}")
    print()

    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Create data
    print("Creating synthetic sequence data...")
    X, y = create_synthetic_sequence_data(
        num_samples=config['num_samples'],
        seq_len=config['seq_len'],
        vocab_size=config['vocab_size'],
        num_classes=config['num_classes']
    )

    # Split data
    train_size = int(0.8 * len(X))
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    train_dataset = TensorDataset(X_train, y_train)
    val_dataset = TensorDataset(X_val, y_val)

    train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False)

    print(f"Training samples: {len(X_train)}")
    print(f"Validation samples: {len(X_val)}")

    # Create model
    print(f"\nCreating MoR model (type: {config['mor_type']})...")
    model = SimpleMoRModel(
        vocab_size=config['vocab_size'],
        hidden_size=config['hidden_size'],
        num_classes=config['num_classes'],
        num_layers=config['num_layers'],
        num_recursions=config['num_recursions'],
        num_heads=config['num_heads'],
        max_seq_len=config['max_seq_len'],
        mor_type=config['mor_type'],
        capacity_factor=config['capacity_factor'],
        alpha=config['alpha'],
        dropout=config['dropout']
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Train model
    print(f"\nüèÉ‚Äç‚ôÇÔ∏è Training model for {config['num_epochs']} epochs...")
    start_time = time.time()

    train_losses, val_accuracies, lb_losses = train_mor_model(
        model, train_loader, val_loader,
        num_epochs=config['num_epochs'],
        learning_rate=config['learning_rate']
    )

    training_time = time.time() - start_time
    print(f"\n‚úÖ Training completed in {training_time:.2f} seconds")
    print(f"Final validation accuracy: {val_accuracies[-1]:.2f}%")
    print(f"Best validation accuracy: {max(val_accuracies):.2f}%")

    # Analyze routing
    print("\nüìä Analyzing routing patterns...")
    routing_stats = analyze_routing_patterns(model, val_loader, num_samples=3)

    # Plot results
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.plot(train_losses)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True)

    plt.subplot(1, 3, 2)
    plt.plot(val_accuracies)
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy (%)')
    plt.grid(True)

    plt.subplot(1, 3, 3)
    plt.plot(lb_losses)
    plt.title('Load Balancing Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Load Balancing Loss')
    plt.grid(True)

    plt.tight_layout()
    plt.show()

    print("\nüéâ Experiment completed successfully!")
    print("\nüìà Key Results:")
    print(f"  ‚Ä¢ Final accuracy: {val_accuracies[-1]:.2f}%")
    print(f"  ‚Ä¢ Best accuracy: {max(val_accuracies):.2f}%")
    print(f"  ‚Ä¢ Training time: {training_time:.2f}s")
    print(f"  ‚Ä¢ Parameters: {total_params:,}")
    print(f"  ‚Ä¢ Model type: {config['mor_type']}")
    print(f"  ‚Ä¢ Recursions: {config['num_recursions']}")

    return model, train_losses, val_accuracies, routing_stats

def colab_instructions():
    """Instructions for Google Colab"""
    print("""
üîß GOOGLE COLAB SETUP INSTRUCTIONS:

1. Open a new Google Colab notebook
2. Copy and paste this entire code
3. Run the following command first to ensure you have the required packages:
   !pip install torch matplotlib numpy

4. Then run the main() function:
   model, losses, accuracies, routing_stats = main()

üß† UNDERSTANDING MIXTURE-OF-RECURSIONS:

Key Concepts:
‚Ä¢ Recursive Transformers: Reuse shared layers multiple times for parameter efficiency
‚Ä¢ Adaptive Depth: Different tokens can be processed with different recursion depths
‚Ä¢ Routing Mechanisms: Decide which tokens need more/less computation
‚Ä¢ Load Balancing: Ensure efficient utilization of computational resources

Two Routing Strategies:
1. Expert-Choice: Each recursion step selects top-k tokens to process
2. Token-Choice: Each token is assigned a fixed recursion depth upfront

üî¨ EXPERIMENTS TO TRY:

1. Compare routing strategies:
   config['mor_type'] = 'expert_choice'  # vs 'token_choice'

2. Adjust recursion depth:
   config['num_recursions'] = 2  # vs 3, 4, 5

3. Modify capacity factor (expert-choice only):
   config['capacity_factor'] = 0.5  # vs 0.8, 1.0

4. Change routing strength:
   config['alpha'] = 0.05  # vs 0.1, 0.2

5. Experiment with model size:
   config['hidden_size'] = 256  # vs 128, 512
   config['num_layers'] = 6     # vs 4, 8

üìä ANALYZING RESULTS:
‚Ä¢ Training curves show convergence
‚Ä¢ Load balancing loss indicates routing efficiency
‚Ä¢ Routing patterns reveal token specialization
‚Ä¢ Compare with standard Transformer baselines

Happy experimenting with Mixture-of-Recursions! üöÄ
""")

if __name__ == "__main__":
    model, losses, accuracies, routing_stats = main()